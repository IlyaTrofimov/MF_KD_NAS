{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor as GPR\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, ConstantKernel\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import ttest_ind\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoost, Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/trofim/NAS-Bench-201-v1_1-096897.pth'\n",
    "\n",
    "from nas_201_api import NASBench201API as API\n",
    "# Create an API without the verbose log\n",
    "api = API(path, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "obj = pickle.load(open('nasbench_val.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hf_val = []\n",
    "hf_test = []\n",
    "hf_times = []\n",
    "\n",
    "lf_val = []\n",
    "lf_test = []\n",
    "lf_times = []\n",
    "\n",
    "for index in range(len(api)):\n",
    "    #\n",
    "    # 200 iter\n",
    "    #\n",
    "    info = api.query_meta_info_by_index(index, hp = '200')  # This is an instance of `ArchResults`\n",
    "    res_metrics = info.get_metrics('cifar10-valid', 'x-valid', 199) # This is a dict with metric names as keys\n",
    "    hf_val.append(res_metrics['accuracy'])\n",
    "    res_metrics = info.get_metrics('cifar10', 'ori-test', 199) # This is a dict with metric names as keys\n",
    "    hf_test.append(res_metrics['accuracy'])\n",
    "    \n",
    "    res_metrics = info.get_metrics('cifar10-valid', 'train', 199) # This is a dict with metric names as keys\n",
    "    hf_times.append(res_metrics['all_time'])\n",
    "    \n",
    "    #\n",
    "    # 12 iter\n",
    "    #\n",
    "    info = api.query_meta_info_by_index(index, hp = '12')  # This is an instance of `ArchResults`\n",
    "    res_metrics = info.get_metrics('cifar10-valid', 'x-valid', 11) # This is a dict with metric names as keys\n",
    "    lf_val.append(res_metrics['accuracy'])\n",
    "    res_metrics = info.get_metrics('cifar10', 'ori-test', 11) # This is a dict with metric names as keys\n",
    "    lf_test.append(res_metrics['accuracy'])\n",
    "    \n",
    "    res_metrics = info.get_metrics('cifar10-valid', 'train', 11) # This is a dict with metric names as keys\n",
    "    lf_times.append(res_metrics['all_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stat(lf, hf):\n",
    "    s1 = stats.kendalltau(lf, hf).correlation\n",
    "    s2 = stats.spearmanr(lf, hf).correlation\n",
    "    s3 = stats.pearsonr(lf, hf)[0]\n",
    "\n",
    "    return [s1, s2, s3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91.60666665039064, 94.37333333333333)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(hf_val), max(hf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30720000000000003"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(filter(lambda x : x > 93.96, hf_test)))/len(hf_test)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf_ep1_val = []\n",
    "\n",
    "for index in range(len(api)):    \n",
    "    info = api.query_meta_info_by_index(index, hp = '12')  # This is an instance of `ArchResults`\n",
    "    res_metrics = info.get_metrics('cifar10-valid', 'x-valid', 0) # This is a dict with metric names as keys\n",
    "    lf_ep1_val.append(res_metrics['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(lf_ep1_val, open('lf_ep1_val_c10.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.160959436666964, 0.23530634003795667, 0.6719242274764118]"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_stat(lf_ep1_val, hf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4721006059101123, 0.6532957791513205, 0.46699173790688836]"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_stat(lf_nst_ep1, hf_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump((lf_val, lf_test, hf_val, hf_test), open('acc_nb201_c10.pickle', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
