{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor as GPR\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, ConstantKernel\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import ttest_ind\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pickle\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mfgpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read benchmark data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rnd1k_ep1_nst_arcs = pickle.load(open('lf_kd_ep1_c10.pickle', 'rb'))      # CIFAR10\n",
    "#(lf_val, lf_test, hf_val, hf_test) = pickle.load(open('acc_nb201_c10.pickle', 'rb'))      # CIFAR 10\n",
    "#rnd1k_ep1_nst_arcs = pickle.load(open('lf_kd_ep1.pickle', 'rb'))          # CIFAR100\n",
    "#(lf_val, lf_test, hf_val, hf_test) = pickle.load(open('acc_nb201.pickle', 'rb'))          # CIFAR 100\n",
    "rnd1k_ep1_nst_arcs = pickle.load(open('lf_kd_ep1_imagenet.pickle', 'rb')) # ImageNet16-120\n",
    "(lf_val, lf_test, hf_val, hf_test) = pickle.load(open('acc_nb201_imagenet.pickle', 'rb'))  # ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = pickle.load(gzip.open('nasbench_val.pkl.gz', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to apply this fix to data order\n",
    "n = 1563\n",
    "\n",
    "ys2 = obj['ys'][n:] + obj['ys'][:n]\n",
    "X2 = obj['descriptors'][n:] + obj['descriptors'][:n]\n",
    "gs2 = obj['gs'][n:] + obj['gs'][:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kd_pred = rnd1k_ep1_nst_arcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_adj = []\n",
    "\n",
    "for elem in gs2:\n",
    "    X_adj.append(elem.numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(kd_pred, hf_test, s=1)\n",
    "plt.xlabel('kd_pred')\n",
    "plt.ylabel('hf_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run simulations of NAS methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random search by high-fidelity data (200 epochs for train stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = 100\n",
    "acc_rs_hf = np.zeros(N)\n",
    "sz = N\n",
    "costs_rs_hf = np.cumsum([200]*sz)\n",
    "for trial in range(trials):\n",
    "    points = list(np.random.choice(list(range(len(X2))), size = N, replace = False))\n",
    "    \n",
    "    y_val_points = [hf_val[x] for x in points]\n",
    "    y_points = np.zeros(N)\n",
    "    \n",
    "    for i in range(len(y_points)):\n",
    "        k = np.argmax(y_val_points[0:i+1])\n",
    "        y_points[i] = hf_test[points[k]]\n",
    "    \n",
    "    acc_rs_hf += np.array(y_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random search by low-fidelity data (12 epochs for train stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = 500\n",
    "\n",
    "sz = int(N*200/12)\n",
    "acc_rs_lf = np.zeros(sz)\n",
    "costs_rs_lf = np.cumsum([12]*sz)\n",
    "acc_rs_lf_all_trials = []\n",
    "for trial in tqdm_notebook(range(trials)):\n",
    "    points = list(np.random.choice(list(range(len(X2))), size = sz, replace = False))\n",
    "    \n",
    "    y_val_points = [lf_val[x] for x in points]\n",
    "    y_points = np.zeros(sz)\n",
    "    \n",
    "    k = 0\n",
    "    for i in range(len(y_points)):\n",
    "        if y_val_points[i] > y_val_points[k]:\n",
    "            k = i\n",
    "        y_points[i] = hf_test[points[k]]\n",
    "    \n",
    "    acc_rs_lf += np.array(y_points)\n",
    "    acc_rs_lf_all_trials.append(np.array(y_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_rs_lf_all_trials = np.array(acc_rs_lf_all_trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random search with KD-data (approx time is ~1.5 of training epochs without KD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = 100\n",
    "\n",
    "sz = min([int(N*200/1.5), len(X2)])\n",
    "acc_rs_kd = np.zeros(sz)\n",
    "costs_rs_kd = np.cumsum([1.5]*sz)\n",
    "\n",
    "for trial in tqdm_notebook(range(trials)):\n",
    "    points = list(np.random.choice(list(range(len(X2))), size = sz, replace = False))\n",
    "    \n",
    "    y_val_points = [kd_pred[x] for x in points]\n",
    "    y_points = np.zeros(sz)\n",
    "    \n",
    "    k = 0\n",
    "    for i in range(len(y_points)):\n",
    "        #k = np.argmax(y_val_points[0:i+1])\n",
    "        if y_val_points[i] > y_val_points[k]:\n",
    "            k = i\n",
    "        y_points[i] = hf_test[points[k]]\n",
    "    \n",
    "    acc_rs_kd += np.array(y_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_MFKDGPR(X_scaled = None, lf_data = None, hf_data = None, max_points = None, r=5, \n",
    "                hf_cost=12, lf_cost=1.5, n_init=20):\n",
    "    refit_each = 5\n",
    "    #checked_points = list(np.random.choice(list(range(len(data))), size = n_init, replace = False))\n",
    "    lf_checked_points = list(np.random.choice(list(range(len(lf_data))), size = r*n_init, replace = False))\n",
    "    \n",
    "    hf_checked_points = list(np.random.choice(list(range(len(hf_data))), size = n_init, replace = False))\n",
    "    \n",
    "    costs = []\n",
    "    costs.append(hf_cost + n_init*r*lf_cost) # include initial cost for low-fidelity\n",
    "    for i in range(1, n_init):\n",
    "        costs.append(hf_cost)\n",
    "    \n",
    "    X_train_lf = [X_scaled[idx] for idx in lf_checked_points] \n",
    "    X_train_hf = [X_scaled[idx] for idx in hf_checked_points] \n",
    "    y_train_lf = [lf_data[idx] for idx in lf_checked_points]\n",
    "    y_train_hf = [hf_data[idx] for idx in hf_checked_points]\n",
    "        \n",
    "    composite_kernel = RBF(length_scale=1, length_scale_bounds=(0.001, 100))\n",
    "    composite_kernel = ConstantKernel(1, constant_value_bounds=(0.001, 100)) * composite_kernel\n",
    "    composite_kernel = WhiteKernel(noise_level=1, noise_level_bounds=(0.001, 100)) + composite_kernel\n",
    "    \n",
    "    if r > 0:\n",
    "        model = mfgpr.GaussianProcessCoKriging(\n",
    "                kernel=composite_kernel,\n",
    "                n_restarts_optimizer=1\n",
    "            )\n",
    "    else:\n",
    "        model = GPR(\n",
    "                kernel=composite_kernel,\n",
    "                n_restarts_optimizer=1\n",
    "            )\n",
    "    \n",
    "    \n",
    "        \n",
    "    pbar = tqdm_notebook(total=max_points)\n",
    "    pbar.update(n_init)\n",
    "    \n",
    "    if r > 0:\n",
    "        model.fit(np.array(X_train_lf), np.array(y_train_lf), np.array(X_train_hf), np.array(y_train_hf))\n",
    "    else:\n",
    "        model.fit(np.array(X_train_hf), np.array(y_train_hf))\n",
    "    \n",
    "    while len(hf_checked_points) < max_points:\n",
    "        # low fidelity x r\n",
    "        \n",
    "        # high fidelity\n",
    "        if len(hf_checked_points) % refit_each == 0:\n",
    "            if r > 0:\n",
    "                model.fit(np.array(X_train_lf), np.array(y_train_lf), np.array(X_train_hf), np.array(y_train_hf))\n",
    "            else:\n",
    "                model.fit(np.array(X_train_hf), np.array(y_train_hf))\n",
    "            #model.fit(np.array(X_train_lf), np.array(y_train_lf), np.array(X_train_hf), np.array(y_train_hf))\n",
    "        \n",
    "        X_scaled_part = []\n",
    "        acq_points = list(np.random.choice(list(range(len(hf_data))), size = min([5000, len(hf_data)]), replace = False))\n",
    "\n",
    "\n",
    "        for idx in acq_points:\n",
    "            X_scaled_part.append(X_scaled[idx])\n",
    "\n",
    "        preds, std = model.predict(np.array(X_scaled_part), return_std=True)\n",
    "        preds = np.hstack((preds.reshape(-1, 1), std.reshape(-1, 1))).T\n",
    "\n",
    "        max_score = -np.inf\n",
    "        best_new_idx = None\n",
    "\n",
    "        for i, idx in enumerate(acq_points):            \n",
    "            if idx not in hf_checked_points:\n",
    "                # UCB\n",
    "                score = preds[0][i] + 1 * preds[1][i] \n",
    "\n",
    "                if score > max_score:\n",
    "                    max_score = score\n",
    "                    best_new_idx = idx\n",
    "\n",
    "        if best_new_idx is not None:\n",
    "            hf_checked_points.append(best_new_idx)\n",
    "            costs.append(hf_cost)\n",
    "            y_train_hf.append(hf_data[best_new_idx])\n",
    "            X_train_hf.append(X_scaled[best_new_idx])\n",
    "        pbar.update(1)\n",
    "    return hf_checked_points, y_train_hf, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPR single fidelity with 12 epochs for train stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpr_trials = 100\n",
    "\n",
    "N_lf = 95\n",
    "\n",
    "acc_gpr = np.zeros(N_lf)\n",
    "all_costs = np.zeros(N_lf)\n",
    "acc_gpr_all_trials = []\n",
    "for trial in tqdm_notebook(range(gpr_trials)):\n",
    "    points, y_train, costs = run_MFKDGPR(X_adj, [], [x/100 for x in lf_val], N_lf, \n",
    "                                         hf_cost=12, r=0)\n",
    "    \n",
    "    y_points = np.zeros(N_lf)\n",
    "    \n",
    "    for i in range(len(y_points)):\n",
    "        k = np.argmax(y_train[0:i+1])\n",
    "        y_points[i] = hf_test[points[k]]\n",
    "    \n",
    "    acc_gpr += np.array(y_points)\n",
    "    all_costs += np.array(costs)\n",
    "    acc_gpr_all_trials.append(np.array(y_points))\n",
    "\n",
    "all_costs_gpr = np.cumsum(all_costs/gpr_trials)\n",
    "\n",
    "acc_gpr_all_trials = np.array(acc_gpr_all_trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-fidelity GPR with 12 epochs as high-fidelity source of data and KD as low-fidelity source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfkdgpr_trials = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_lf = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_mfkdgpr = np.zeros(N_lf)\n",
    "all_costs = np.zeros(N_lf)\n",
    "acc_mfkdgpr_all_trials = []\n",
    "for trial in tqdm_notebook(range(mfkdgpr_trials)):\n",
    "    points, y_train, costs = run_MFKDGPR(X_adj, [x/100 for x in kd_pred], [x/100 for x in lf_val], N_lf, \n",
    "                                         hf_cost=12)\n",
    "    \n",
    "    y_points = np.zeros(N_lf)\n",
    "    \n",
    "    for i in range(len(y_points)):\n",
    "        k = np.argmax(y_train[0:i+1])\n",
    "        y_points[i] = hf_test[points[k]]\n",
    "    \n",
    "    acc_mfkdgpr += np.array(y_points)\n",
    "    all_costs += np.array(costs)\n",
    "    acc_mfkdgpr_all_trials.append(np.array(y_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_mfkdgpr_all_trials = np.array(acc_mfkdgpr_all_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_costs_kd_lf = np.cumsum(all_costs/mfkdgpr_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5), dpi=100)\n",
    "\n",
    "plt.plot(costs_rs_hf, acc_rs_hf/trials, label = 'Random Search HF', color=plt.cm.Greys(0.8))\n",
    "plt.plot(costs_rs_lf, acc_rs_lf_all_trials.mean(axis=0), label = 'Random Search LF', color=plt.cm.Greys(0.6))\n",
    "plt.plot(costs_rs_kd, acc_rs_kd/trials, label = 'Random Search KD', color=plt.cm.Greys(0.4))\n",
    "plt.plot(all_costs_kd_lf, acc_mfkdgpr_all_trials.mean(axis=0), label = 'MF-KD-GPR (KD+LF r=5)')\n",
    "plt.plot(all_costs_gpr, acc_gpr_all_trials.mean(axis=0), label = 'GPR', color='green')\n",
    "plt.xlim([0, 1000])\n",
    "plt.xlabel('num.epochs')\n",
    "plt.ylabel('test accuracy')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
